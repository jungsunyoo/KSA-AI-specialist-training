{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['effort', 'balance', 'preferences', 'intellectual', 'property', 'attendees', 'institutions', 'open', 'scicomm', 'presenters', 'exhibitors', 'sfn19', 'choice', 'permit', 'photography', 'recording', 'work', 'on-site']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "vocab=Counter()\n",
    "\n",
    "sentences=[]\n",
    "stop_words=set(stopwords.words('english'))\n",
    "text=\"In an effort to balance the preferences and intellectual property of attendees and their institutions with open #scicomm, presenters and exhibitors at #SfN19 will have the choice to permit photography and recording of their work on-site.\"\n",
    "# text = \"A barber is a person. a barber is good erson. a berber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word.\"\n",
    "text=sent_tokenize(text)\n",
    "for i in text:\n",
    "    sentence=word_tokenize(i)\n",
    "    result=[]\n",
    "    \n",
    "    for word in sentence: \n",
    "        word = word.lower()\n",
    "        if word not in stop_words:\n",
    "            if len(word) > 2: \n",
    "                result.append(word)\n",
    "                vocab[word]=vocab[word]+1\n",
    "    sentences.append(result)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'to': 2, 'the': 3, 'of': 4, 'their': 5, 'in': 6, 'an': 7, 'effort': 8, 'balance': 9, 'preferences': 10, 'intellectual': 11, 'property': 12, 'attendees': 13, 'institutions': 14, 'with': 15, 'open': 16, 'scicomm': 17, 'presenters': 18, 'exhibitors': 19, 'at': 20, 'sfn19': 21, 'will': 22, 'have': 23, 'choice': 24, 'permit': 25, 'photography': 26, 'recording': 27, 'work': 28, 'on': 29, 'site': 30}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "test=[\"A barber is a person. a barber is good erson. a berber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word.\"]\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(text)\n",
    "\n",
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('in', 1), ('an', 1), ('effort', 1), ('to', 2), ('balance', 1), ('the', 2), ('preferences', 1), ('and', 4), ('intellectual', 1), ('property', 1), ('of', 2), ('attendees', 1), ('their', 2), ('institutions', 1), ('with', 1), ('open', 1), ('scicomm', 1), ('presenters', 1), ('exhibitors', 1), ('at', 1), ('sfn19', 1), ('will', 1), ('have', 1), ('choice', 1), ('permit', 1), ('photography', 1), ('recording', 1), ('work', 1), ('on', 1), ('site', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(t.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 7, 8, 2, 9, 3, 10, 1, 11, 12, 4, 13, 1, 5, 14, 15, 16, 17, 18, 1, 19, 20, 21, 22, 23, 3, 24, 2, 25, 26, 1, 27, 4, 5, 28, 29, 30]]\n"
     ]
    }
   ],
   "source": [
    "print(t.texts_to_sequences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW 만들기 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "okt=Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = re.sub(\"(\\.)\", \"\", \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")\n",
    "# 정규 표현식을 통해 온점을 제거하는 정제 작업입니다.\n",
    "token = okt.morphs(token)\n",
    "# OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index={}\n",
    "bow=[]\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index)\n",
    "        bow.insert(len(word2index)-1,1)\n",
    "    else: \n",
    "        index=word2index.get(voca)\n",
    "        bow[index]=bow[index]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록한다. \n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (27,28,29,30,31,32,33,34,35,36,38,40,41,44,47,48,50,51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('폭염 (1990.01.01-2018.07.31)_1.csv', sep=',', quotechar='\"', error_bad_lines=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29982, 53)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['주소', '일자', '언론사', '기고자', '제목', '통합 분류1', '통합 분류2', '통합 분류3',\n",
       "       '사건/사고 분류1', '사건/사고 분류2', '사건/사고 분류3', '개체명(인물)', '개체명(지역)',\n",
       "       '개체명(기업/기관)', '키워드', '특성추출', '본문', '원본주소', 'Unnamed: 18', 'Unnamed: 19',\n",
       "       'Unnamed: 20', 'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23',\n",
       "       'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27',\n",
       "       'Unnamed: 28', 'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31',\n",
       "       'Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35',\n",
       "       'Unnamed: 36', 'Unnamed: 37', 'Unnamed: 38', 'Unnamed: 39',\n",
       "       'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43',\n",
       "       'Unnamed: 44', 'Unnamed: 45', 'Unnamed: 46', 'Unnamed: 47',\n",
       "       'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50', 'Unnamed: 51',\n",
       "       'Unnamed: 52'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    독일의 어느 시인은 ‘경이롭게 아름다운 5월에’라고 노래했지만,우리에게는 5월의 경...\n",
       "1    水原 32도까지…11일께 한풀 꺾일 듯나흘째 초여름 무더위가 계속되고 있다.이번 더...\n",
       "2    ◎어제하오 2∼3시사이연일 계속되는 무더위로 전력소비가 급증,시간당 최대전력사용량이...\n",
       "3    군당국은 11일 행군훈련중이던 방위병 3명이 열사병으로 숨진것은 혹서기안전관리규정을...\n",
       "4    민주화는 궁극엔 인간화와 통한다. 사람이 사람대접을 받으며 사람답게 살자는 것이다....\n",
       "Name: 본문, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['본문'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_nouns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-84fe3273d151>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKomoran\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_nouns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtdm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'본문'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_nouns' is not defined"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=1000, tokenizer=get_nouns)\n",
    "tdm = cv.fit_transform(df['본문'])\n",
    "cv.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'you know I want your love', \n",
    "    'I like you', \n",
    "    'what should I do',\n",
    "]\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('review.gz', <http.client.HTTPMessage at 0x1f136bdf1d0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "urlretrieve('http://doc.mindscale.kr/km/unstructured/review.gz', 'review.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras2",
   "language": "python",
   "name": "keras2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
