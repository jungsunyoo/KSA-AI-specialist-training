{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 데이터셋을 합축 해제한 디렉터리 경로\n",
    "# original_dataset_dir = './fruits-fresh-and-rotten-for-classification/dataset'\n",
    "tf.set_random_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './CNN'\n",
    "# if os.path.exists(base_dir): # 반복적인 실행을 위해 디렉터리를 삭제합니다.\n",
    "#     shutil.rmtree(base_dir) # 이 코드는 책에 포함되어 있지 않습니다.\n",
    "# os.mkdir(base_dir)\n",
    "# base_dir = './datasets/datasets'\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "validation_freshapple_dir = os.path.join(validation_dir, 'freshapples')\n",
    "validation_freshbanana_dir = os.path.join(validation_dir, 'freshbanana')\n",
    "validation_freshorange_dir = os.path.join(validation_dir, 'freshoranges')\n",
    "validation_rottenapple_dir = os.path.join(validation_dir, 'rottenapples')\n",
    "validation_rottenbanana_dir = os.path.join(validation_dir, 'rottenbanana')\n",
    "validation_rottenorange_dir = os.path.join(validation_dir, 'rottenoranges')\n",
    "\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "train_freshapple_dir = os.path.join(train_dir, 'freshapples')\n",
    "train_freshbanana_dir = os.path.join(train_dir, 'freshbanana')\n",
    "train_freshorange_dir = os.path.join(train_dir, 'freshoranges')\n",
    "train_rottenapple_dir = os.path.join(train_dir, 'rottenapples')\n",
    "train_rottenbanana_dir = os.path.join(train_dir, 'rottenbanana')\n",
    "train_rottenorange_dir = os.path.join(train_dir, 'rottenoranges')\n",
    "\n",
    "\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "test_freshapple_dir = os.path.join(test_dir, 'freshapples')\n",
    "test_freshbanana_dir = os.path.join(test_dir, 'freshbanana')\n",
    "test_freshorange_dir = os.path.join(test_dir, 'freshoranges')\n",
    "test_rottenapple_dir = os.path.join(test_dir, 'rottenapples')\n",
    "test_rottenbanana_dir = os.path.join(test_dir, 'rottenbanana')\n",
    "test_rottenorange_dir = os.path.join(test_dir, 'rottenoranges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 신선한 사과 이미지 전체 개수: 1000\n",
      "훈련용 신선한 바나나 이미지 전체 개수: 1000\n",
      "훈련용 신선한 오렌지 이미지 전체 개수: 1000\n",
      "훈련용 상한 사과 이미지 전체 개수: 1000\n",
      "훈련용 상한 바나나 이미지 전체 개수: 1000\n",
      "훈련용 상한 오렌지 이미지 전체 개수: 1000\n",
      "검증용 신선한 사과 이미지 전체 개수: 380\n",
      "검증용 신선한 바나나 이미지 전체 개수: 380\n",
      "검증용 신선한 오렌지 이미지 전체 개수: 380\n",
      "검증용 상한 사과 이미지 전체 개수: 380\n",
      "검증용 상한 바나나 이미지 전체 개수: 380\n",
      "검증용 상한 오렌지 이미지 전체 개수: 380\n",
      "테스트용 신선한 사과 이미지 전체 개수: 380\n",
      "테스트용 신선한 바나나 이미지 전체 개수: 380\n",
      "테스트용 신선한 오렌지 이미지 전체 개수: 380\n",
      "테스트용 상한 사과 이미지 전체 개수: 380\n",
      "테스트용 상한 바나나 이미지 전체 개수: 380\n",
      "테스트용 상한 오렌지 이미지 전체 개수: 380\n"
     ]
    }
   ],
   "source": [
    "print('훈련용 신선한 사과 이미지 전체 개수:', len(os.listdir(train_freshapple_dir)))\n",
    "print('훈련용 신선한 바나나 이미지 전체 개수:', len(os.listdir(train_freshbanana_dir)))\n",
    "print('훈련용 신선한 오렌지 이미지 전체 개수:', len(os.listdir(train_freshorange_dir)))\n",
    "print('훈련용 상한 사과 이미지 전체 개수:', len(os.listdir(train_rottenapple_dir)))\n",
    "print('훈련용 상한 바나나 이미지 전체 개수:', len(os.listdir(train_rottenbanana_dir)))\n",
    "print('훈련용 상한 오렌지 이미지 전체 개수:', len(os.listdir(train_rottenorange_dir)))\n",
    "\n",
    "print('검증용 신선한 사과 이미지 전체 개수:', len(os.listdir(validation_freshapple_dir)))\n",
    "print('검증용 신선한 바나나 이미지 전체 개수:', len(os.listdir(validation_freshbanana_dir)))\n",
    "print('검증용 신선한 오렌지 이미지 전체 개수:', len(os.listdir(validation_freshorange_dir)))\n",
    "print('검증용 상한 사과 이미지 전체 개수:', len(os.listdir(validation_rottenapple_dir)))\n",
    "print('검증용 상한 바나나 이미지 전체 개수:', len(os.listdir(validation_rottenbanana_dir)))\n",
    "print('검증용 상한 오렌지 이미지 전체 개수:', len(os.listdir(validation_rottenorange_dir)))\n",
    "\n",
    "print('테스트용 신선한 사과 이미지 전체 개수:', len(os.listdir(test_freshapple_dir)))\n",
    "print('테스트용 신선한 바나나 이미지 전체 개수:', len(os.listdir(test_freshbanana_dir)))\n",
    "print('테스트용 신선한 오렌지 이미지 전체 개수:', len(os.listdir(test_freshorange_dir)))\n",
    "print('테스트용 상한 사과 이미지 전체 개수:', len(os.listdir(test_rottenapple_dir)))\n",
    "print('테스트용 상한 바나나 이미지 전체 개수:', len(os.listdir(test_rottenbanana_dir)))\n",
    "print('테스트용 상한 오렌지 이미지 전체 개수:', len(os.listdir(test_rottenorange_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네트워크 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 images belonging to 6 classes.\n",
      "Found 2280 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 모든 이미지를 1/255로 스케일을 조정합니다\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # 타깃 디렉터리\n",
    "        train_dir,\n",
    "        # 모든 이미지를 150 × 150 크기로 바꿉니다\n",
    "        target_size=(150, 150),\n",
    "        batch_size=100,\n",
    "        # binary_crossentropy 손실을 사용하기 때문에 이진 레이블이 필요합니다\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=100,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배치 데이터 크기: (100, 150, 150, 3)\n",
      "배치 레이블 크기: (100, 6)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('배치 데이터 크기:', data_batch.shape)\n",
    "    print('배치 레이블 크기:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0718 15:28:00.994741 14200 deprecation_wrapper.py:119] From C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), padding = 'same', activation='linear',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(keras.layers.advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(layers.Conv2D(32,(3,3), padding='same', activation='linear'))\n",
    "model.add(layers.advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32,(3,3), padding='same', activation='linear'))\n",
    "model.add(keras.layers.advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(layers.Conv2D(32,(3,3), padding='same', activation='linear'))\n",
    "model.add(keras.layers.advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32,(3,3), padding='same', activation='linear'))\n",
    "model.add(keras.layers.advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(layers.Conv2D(32,(3,3), padding='same', activation='linear'))\n",
    "model.add(keras.layers.advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3),activation='relu',input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(keras.layers.advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(layers.Conv2D(64,(3,3), padding='same', activation='linear'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(keras.layers.advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(layers.Conv2D(128,(3,3), padding='same', activation='linear'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(keras.layers.advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(layers.Conv2D(256,(3,3), padding='same', activation='linear'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten()) \n",
    "model.add(layers.Dropout(0.5)) # 0.5를 죽임\n",
    "\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "\n",
    "# optimizer\n",
    "from keras import optimizers\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=150,\n",
    "      epochs=50,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from keras import optimizers\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=60,\n",
    "      epochs=5,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('fruits_val.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras2",
   "language": "python",
   "name": "keras2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
